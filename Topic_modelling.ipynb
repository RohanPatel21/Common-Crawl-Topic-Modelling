{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6384e7cf-dbcf-4ce9-bedb-91aaaa53ecda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading words: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     Hostname mismatch, certificate is not valid for\n",
      "[nltk_data]     'raw.githubusercontent.com'. (_ssl.c:1131)>\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 08:22:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/19 08:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/19 08:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/05/19 08:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/05/19 08:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/05/19 08:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/05/19 08:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/05/19 08:22:59 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.204.93.70:4047\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa63c7c4130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import os\n",
    "import warcio\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "from nltk.corpus import words\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum as sql_sum\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, explode, lit,array\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, ArrayType\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "from scipy.sparse import csr_matrix\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"myApp\") \\\n",
    "    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf2a1ac-301e-48f7-8dee-5ef71b8d4367",
   "metadata": {},
   "source": [
    "#### Defining a schema and the corresponding data types to store the web crawl data.\n",
    "### Attributions :\n",
    "##### https://groups.google.com/g/common-crawl/c/yEP1Lt1_B4E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bdf1592-e565-4b7c-b907-d3d24da23ec4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"WARC-Type\", StringType(), True),\n",
    "    StructField(\"WARC-Target-URI\", StringType(), True),\n",
    "    StructField(\"WARC-Date\", StringType(), True),\n",
    "    StructField(\"WARC-Record-ID\", StringType(), True),\n",
    "    StructField(\"WARC-Refers-To\", StringType(), True),\n",
    "    StructField(\"WARC-Block-Digest\", StringType(), True),\n",
    "    StructField(\"WARC-Identified-Content-Language\", StringType(), True),\n",
    "    StructField(\"Content-Type\", StringType(), True),\n",
    "    StructField(\"Content-Length\", IntegerType(), True),\n",
    "    StructField(\"Content\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad46ee4-5805-4428-a3f8-071397e44c78",
   "metadata": {},
   "source": [
    "#### Reading the WARC data from the file .wet file, and storing the headers and content in a list 'records'.\n",
    "### Attributions :\n",
    "##### https://www.programcreek.com/python/example/127575/warcio.archiveiterator.ArchiveIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e926af9b-3687-4b24-a6ef-5f02779aba48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "records = []\n",
    "with open('crawl.wet.gz', 'rb') as data:\n",
    "    for i in warcio.archiveiterator.ArchiveIterator(data):\n",
    "        headers = dict(i.rec_headers.headers)\n",
    "        headers['Content-Length'] = int(headers['Content-Length'])\n",
    "        headers['Content'] = i.raw_stream.read().decode('utf-8')\n",
    "        records.append(headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c1d35a-8555-44ce-9e72-fa2254181c1c",
   "metadata": {},
   "source": [
    "#### Creating a spark dataframe out of the list records in the format of the schema built above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15f3963-c582-488d-bce0-d2cf2960c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(records, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a21baf7-f118-4e61-96e2-df7fe7efe393",
   "metadata": {},
   "source": [
    "#### Tokenizing the 'Content' column in the DataFrame using a regex pattern that matches non-word characters, and storing the tokens in a new column 'words'.\n",
    "#### Attributions :\n",
    "##### https://www.sparkitecture.io/natural-language-processing/data-preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63fcb738-b217-46aa-b9e6-84daa766cdf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexTokenizer(inputCol=\"Content\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "tokenized_df = tokenizer.transform(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc704822-6c34-4e78-b55f-c8eeeebeec28",
   "metadata": {},
   "source": [
    "#### Removing stopwords from the 'words' column, and storing the filtered words in a new column 'filtered'.\n",
    "#### Attributions :\n",
    "##### https://ashokpalivela.medium.com/multi-class-text-classification-using-spark-ml-in-python-b8d2a6545cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d62ef51-b1ad-4cd6-929a-16eeb61dc07e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "filtered_df = stopwords_remover.transform(tokenized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365ad2fc-9195-47de-bff7-e4ec9f04aef6",
   "metadata": {},
   "source": [
    "#### Instantiated a CountVectorizer with the specified parameters to vectorize the 'english_words' column, and stored the results in a new column 'rawFeatures'.  Additionally, calculated the TF-IDF values of the words in the 'rawFeatures' column, and stored the results in a new column 'features'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c0f9c8-8e12-485a-897e-7961e5803622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "english_words = set(words.words())\n",
    "def english(words):\n",
    "    return [word for word in words if word.lower() in english_words]\n",
    "english_udf = udf(english, ArrayType(StringType()))\n",
    "english_df = filtered_df.withColumn(\"english_words\", english_udf(\"filtered\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9256fd-fa55-4e16-867f-4febc55a3f13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 08:23:14 WARN TaskSetManager: Stage 0 contains a task of very large size (41255 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/05/19 08:24:21 WARN TaskSetManager: Stage 4 contains a task of very large size (41255 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 8) / 8]\r"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(inputCol=\"english_words\", outputCol=\"rawFeatures\", vocabSize=5000, minDF=5.0)\n",
    "vectorized_df = vectorizer.fit(english_df).transform(english_df)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idf_model = idf.fit(vectorized_df)\n",
    "tfidf_vectors = idf_model.transform(vectorized_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c889a65f-47ff-4686-8b99-f131f952972a",
   "metadata": {},
   "source": [
    "#### Training an LDA model on the 'features' column, with the specified number of topics and iterations and extracting the top terms of each topicfrom the LDA model, and store them in a new DataFrame 'topics_with_terms'.\n",
    "### Attributions :\n",
    "##### https://medium.com/analytics-vidhya/distributed-topic-modelling-using-spark-nlp-and-spark-mllib-lda-6db3f06a4da3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71748e-2925-422c-8273-cc32aabe6b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_topics = 20\n",
    "lda = LDA(k=num_topics, maxIter=10)\n",
    "lda_model = lda.fit(tfidf_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f6b1c-2aa8-4b49-898f-69d0c16adca8",
   "metadata": {},
   "source": [
    "#### extracting the vocabulary that the model has learned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e4f02-fc16-4170-bb13-1216a009565a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer_model = vectorizer.fit(english_df)\n",
    "vocab = vectorizer_model.vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974f732-e05e-49b4-ac22-6597a46ad7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topics = lda_model.describeTopics(maxTermsPerTopic=10)\n",
    "topic_terms_udf = udf(lambda indices: [vocab[i] for i in indices], ArrayType(StringType()))\n",
    "topics_with_terms = topics.withColumn(\"topic_terms\", topic_terms_udf(\"termIndices\"))\n",
    "topics_with_terms.select(\"topic\", \"topic_terms\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce2bb6-14c5-42fe-826d-10bdf4eaaf1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "topics_with_terms.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aa7dce-8321-40bf-8568-6167816667e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "top_topics = topics_with_terms.select(\"topic\", \"topic_terms\").limit(n).collect()\n",
    "top_topics_list = [(row.topic, row.topic_terms) for row in top_topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a6ea9a-68cc-4d50-b731-af6f1e24a6bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The top\", n, \"topics are:\", top_topics_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1373fa9f-de31-4b27-bd3d-74616f22bffa",
   "metadata": {},
   "source": [
    "#### Defining a function to convert a sparse vector from the 'rawFeatures' column to a SciPy sparse matrix. I have converted each vector in the 'rawFeatures' column to a SciPy sparse matrix, and stored them in a list.\n",
    "### Attributions :\n",
    "##### https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d98af-4676-4170-ac89-930a6fa5bc3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scipy_sparse(vector):\n",
    "    return csr_matrix((vector.values, vector.indices, [0, len(vector.values)]), shape=(1, len(vector)))\n",
    "sparse_matrices = [scipy_sparse(vector) for vector in vectorized_df.select(\"rawFeatures\").rdd.map(lambda x: x[0]).collect()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1861e6f-3708-408c-9d50-26846d238fac",
   "metadata": {},
   "source": [
    "#### Converting scipy sparse matrices to a gensim corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1443151f-4c65-416b-9a80-087b39d50b50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gensim_corpus(sparse_matrices):\n",
    "    for matrix in sparse_matrices:\n",
    "        yield [(i, float(matrix[0, i])) for i in matrix.indices]\n",
    "corpus = list(gensim_corpus(sparse_matrices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c805eaf-c9a8-4d16-9569-acd6390f2bc5",
   "metadata": {},
   "source": [
    "#### Creating a Gensim Dictionary from the vocabulary of the CountVectorizer and filtering out the outliers.\n",
    "### Attributions:\n",
    "##### https://www.tutorialspoint.com/gensim/gensim_creating_a_dictionary.htm\n",
    "##### https://tedboy.github.io/nlps/generated/generated/gensim.corpora.Dictionary.filter_extremes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a23d6-10d9-474b-9a23-e22dabf7cf01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gensim_vocab = gensim.corpora.Dictionary([vectorizer_model.vocabulary])\n",
    "gensim_vocab.filter_extremes(no_below=1, no_above=1.0, keep_n=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d0585-4a9a-439f-bade-afbf0f65b032",
   "metadata": {},
   "source": [
    "#### Training a Gensim LDA model on the Gensim corpus with the specified number of topics and preparing the Gensim LDA model's data for visualization using the pyLDAvis library.\n",
    "### Attributions :\n",
    "##### https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a4edcd-495a-4950-adab-53618fe17068",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "gensim_lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=gensim_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9d6dae-2477-4b0f-b419-e3a9d71fd7c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vis = gensimvis.prepare(gensim_lda_model, corpus, gensim_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc367138-f2d0-4983-ba8a-9338229a8e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyLDAvis.display(vis)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
